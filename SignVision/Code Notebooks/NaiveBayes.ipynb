{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9167cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed90e2",
   "metadata": {},
   "source": [
    "# Na√Øve Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4b3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057f7ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pankaj\\AppData\\Local\\Temp\\ipykernel_23856\\789716297.py:2: DtypeWarning: Columns (1024) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('procesed_imgdata_32x32.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('procesed_imgdata_32x32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efcbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = data.iloc[:,:-3]\n",
    "y = data[\"labelsEnc\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc62e057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for alpha=0.01:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       291\n",
      "           1       0.73      0.59      0.65       292\n",
      "           2       0.67      0.73      0.70       309\n",
      "           3       0.78      0.83      0.80       323\n",
      "           4       0.76      0.68      0.72       308\n",
      "           5       0.72      0.78      0.75       298\n",
      "           6       0.61      0.52      0.56       307\n",
      "           7       0.59      0.84      0.69       361\n",
      "           8       0.98      0.91      0.95       311\n",
      "           9       0.94      0.68      0.79       340\n",
      "          10       0.53      0.48      0.50       298\n",
      "          11       0.43      0.38      0.40       322\n",
      "          12       0.53      0.44      0.48       331\n",
      "          13       0.66      0.29      0.41       326\n",
      "          14       0.32      0.20      0.25       317\n",
      "          15       0.14      0.31      0.20       315\n",
      "          16       0.36      0.38      0.37       325\n",
      "          17       0.34      0.08      0.12       313\n",
      "          18       0.47      0.44      0.45       319\n",
      "          19       0.44      0.09      0.14       312\n",
      "          20       0.46      0.40      0.43       290\n",
      "          21       0.51      0.28      0.36       307\n",
      "          22       0.15      0.15      0.15       326\n",
      "          23       0.22      0.19      0.20       309\n",
      "          24       0.56      0.65      0.60       317\n",
      "          25       0.40      0.70      0.51       319\n",
      "          26       0.10      0.08      0.09       299\n",
      "          27       0.76      0.41      0.53       310\n",
      "          28       0.49      0.80      0.60       316\n",
      "          29       0.19      0.04      0.07       285\n",
      "          30       0.10      0.14      0.12       334\n",
      "          31       0.41      0.28      0.33       317\n",
      "          32       0.17      0.19      0.18       322\n",
      "          33       0.21      0.09      0.12       294\n",
      "          34       0.09      0.33      0.15       321\n",
      "          35       0.27      0.38      0.32       312\n",
      "          36       0.19      0.04      0.07       319\n",
      "          37       0.35      0.29      0.31       291\n",
      "          38       0.24      0.36      0.29       340\n",
      "\n",
      "    accuracy                           0.42     12246\n",
      "   macro avg       0.46      0.42      0.42     12246\n",
      "weighted avg       0.46      0.42      0.42     12246\n",
      "\n",
      "Classification report for alpha=0.1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       291\n",
      "           1       0.73      0.59      0.65       292\n",
      "           2       0.67      0.73      0.70       309\n",
      "           3       0.78      0.83      0.80       323\n",
      "           4       0.76      0.68      0.72       308\n",
      "           5       0.72      0.78      0.75       298\n",
      "           6       0.61      0.52      0.56       307\n",
      "           7       0.59      0.84      0.69       361\n",
      "           8       0.98      0.91      0.95       311\n",
      "           9       0.94      0.68      0.79       340\n",
      "          10       0.53      0.48      0.50       298\n",
      "          11       0.43      0.38      0.40       322\n",
      "          12       0.53      0.44      0.48       331\n",
      "          13       0.66      0.29      0.41       326\n",
      "          14       0.32      0.20      0.25       317\n",
      "          15       0.14      0.31      0.20       315\n",
      "          16       0.36      0.38      0.37       325\n",
      "          17       0.34      0.08      0.12       313\n",
      "          18       0.47      0.44      0.45       319\n",
      "          19       0.44      0.09      0.14       312\n",
      "          20       0.46      0.40      0.43       290\n",
      "          21       0.51      0.28      0.36       307\n",
      "          22       0.15      0.15      0.15       326\n",
      "          23       0.22      0.19      0.21       309\n",
      "          24       0.56      0.65      0.60       317\n",
      "          25       0.40      0.70      0.51       319\n",
      "          26       0.10      0.08      0.09       299\n",
      "          27       0.76      0.41      0.53       310\n",
      "          28       0.48      0.80      0.60       316\n",
      "          29       0.19      0.04      0.07       285\n",
      "          30       0.10      0.14      0.12       334\n",
      "          31       0.41      0.28      0.33       317\n",
      "          32       0.17      0.19      0.18       322\n",
      "          33       0.21      0.09      0.12       294\n",
      "          34       0.09      0.33      0.15       321\n",
      "          35       0.27      0.38      0.32       312\n",
      "          36       0.19      0.04      0.07       319\n",
      "          37       0.35      0.29      0.31       291\n",
      "          38       0.24      0.36      0.29       340\n",
      "\n",
      "    accuracy                           0.42     12246\n",
      "   macro avg       0.46      0.42      0.42     12246\n",
      "weighted avg       0.46      0.42      0.42     12246\n",
      "\n",
      "Classification report for alpha=1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       291\n",
      "           1       0.73      0.58      0.65       292\n",
      "           2       0.67      0.73      0.70       309\n",
      "           3       0.78      0.83      0.80       323\n",
      "           4       0.76      0.68      0.72       308\n",
      "           5       0.72      0.78      0.75       298\n",
      "           6       0.62      0.52      0.56       307\n",
      "           7       0.58      0.84      0.69       361\n",
      "           8       0.98      0.91      0.95       311\n",
      "           9       0.94      0.68      0.79       340\n",
      "          10       0.53      0.48      0.50       298\n",
      "          11       0.43      0.38      0.40       322\n",
      "          12       0.53      0.44      0.48       331\n",
      "          13       0.66      0.29      0.41       326\n",
      "          14       0.32      0.20      0.25       317\n",
      "          15       0.14      0.31      0.20       315\n",
      "          16       0.36      0.38      0.37       325\n",
      "          17       0.34      0.08      0.12       313\n",
      "          18       0.47      0.44      0.45       319\n",
      "          19       0.44      0.09      0.14       312\n",
      "          20       0.45      0.40      0.43       290\n",
      "          21       0.51      0.28      0.36       307\n",
      "          22       0.15      0.15      0.15       326\n",
      "          23       0.22      0.19      0.21       309\n",
      "          24       0.56      0.65      0.60       317\n",
      "          25       0.40      0.70      0.51       319\n",
      "          26       0.09      0.07      0.08       299\n",
      "          27       0.76      0.41      0.53       310\n",
      "          28       0.48      0.80      0.60       316\n",
      "          29       0.19      0.04      0.07       285\n",
      "          30       0.10      0.14      0.12       334\n",
      "          31       0.41      0.28      0.33       317\n",
      "          32       0.16      0.19      0.17       322\n",
      "          33       0.21      0.09      0.12       294\n",
      "          34       0.09      0.33      0.15       321\n",
      "          35       0.27      0.38      0.32       312\n",
      "          36       0.19      0.04      0.07       319\n",
      "          37       0.35      0.29      0.31       291\n",
      "          38       0.24      0.36      0.29       340\n",
      "\n",
      "    accuracy                           0.42     12246\n",
      "   macro avg       0.46      0.42      0.42     12246\n",
      "weighted avg       0.46      0.42      0.42     12246\n",
      "\n",
      "Classification report for alpha=5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       291\n",
      "           1       0.72      0.57      0.64       292\n",
      "           2       0.68      0.72      0.70       309\n",
      "           3       0.78      0.84      0.81       323\n",
      "           4       0.76      0.69      0.72       308\n",
      "           5       0.71      0.77      0.74       298\n",
      "           6       0.62      0.52      0.56       307\n",
      "           7       0.58      0.85      0.69       361\n",
      "           8       0.98      0.91      0.95       311\n",
      "           9       0.94      0.68      0.79       340\n",
      "          10       0.53      0.48      0.51       298\n",
      "          11       0.43      0.37      0.40       322\n",
      "          12       0.54      0.44      0.48       331\n",
      "          13       0.66      0.29      0.40       326\n",
      "          14       0.32      0.20      0.25       317\n",
      "          15       0.14      0.31      0.20       315\n",
      "          16       0.36      0.38      0.37       325\n",
      "          17       0.34      0.08      0.13       313\n",
      "          18       0.47      0.44      0.45       319\n",
      "          19       0.47      0.09      0.15       312\n",
      "          20       0.46      0.40      0.43       290\n",
      "          21       0.52      0.28      0.36       307\n",
      "          22       0.15      0.16      0.15       326\n",
      "          23       0.23      0.20      0.21       309\n",
      "          24       0.57      0.65      0.61       317\n",
      "          25       0.40      0.70      0.51       319\n",
      "          26       0.09      0.07      0.08       299\n",
      "          27       0.76      0.41      0.53       310\n",
      "          28       0.48      0.81      0.60       316\n",
      "          29       0.19      0.04      0.07       285\n",
      "          30       0.10      0.14      0.11       334\n",
      "          31       0.41      0.28      0.33       317\n",
      "          32       0.16      0.19      0.18       322\n",
      "          33       0.21      0.09      0.12       294\n",
      "          34       0.09      0.33      0.15       321\n",
      "          35       0.27      0.38      0.32       312\n",
      "          36       0.20      0.04      0.07       319\n",
      "          37       0.34      0.29      0.31       291\n",
      "          38       0.23      0.36      0.29       340\n",
      "\n",
      "    accuracy                           0.42     12246\n",
      "   macro avg       0.46      0.42      0.42     12246\n",
      "weighted avg       0.46      0.42      0.42     12246\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for alpha=10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       291\n",
      "           1       0.72      0.57      0.63       292\n",
      "           2       0.67      0.72      0.69       309\n",
      "           3       0.77      0.85      0.81       323\n",
      "           4       0.76      0.69      0.72       308\n",
      "           5       0.70      0.78      0.74       298\n",
      "           6       0.63      0.52      0.57       307\n",
      "           7       0.58      0.85      0.69       361\n",
      "           8       0.98      0.91      0.95       311\n",
      "           9       0.93      0.68      0.79       340\n",
      "          10       0.54      0.48      0.51       298\n",
      "          11       0.43      0.37      0.40       322\n",
      "          12       0.54      0.44      0.48       331\n",
      "          13       0.64      0.27      0.38       326\n",
      "          14       0.31      0.20      0.24       317\n",
      "          15       0.15      0.31      0.20       315\n",
      "          16       0.36      0.37      0.36       325\n",
      "          17       0.34      0.08      0.13       313\n",
      "          18       0.47      0.44      0.45       319\n",
      "          19       0.48      0.09      0.15       312\n",
      "          20       0.46      0.41      0.43       290\n",
      "          21       0.53      0.28      0.37       307\n",
      "          22       0.15      0.16      0.16       326\n",
      "          23       0.23      0.21      0.22       309\n",
      "          24       0.57      0.65      0.61       317\n",
      "          25       0.40      0.70      0.51       319\n",
      "          26       0.09      0.07      0.08       299\n",
      "          27       0.76      0.41      0.53       310\n",
      "          28       0.48      0.81      0.60       316\n",
      "          29       0.19      0.04      0.07       285\n",
      "          30       0.10      0.14      0.12       334\n",
      "          31       0.41      0.27      0.33       317\n",
      "          32       0.16      0.19      0.18       322\n",
      "          33       0.22      0.09      0.13       294\n",
      "          34       0.09      0.33      0.15       321\n",
      "          35       0.27      0.39      0.32       312\n",
      "          36       0.20      0.04      0.07       319\n",
      "          37       0.34      0.29      0.31       291\n",
      "          38       0.23      0.36      0.28       340\n",
      "\n",
      "    accuracy                           0.42     12246\n",
      "   macro avg       0.46      0.42      0.42     12246\n",
      "weighted avg       0.46      0.42      0.42     12246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "for alpha in [0.01, 0.1, 1, 5, 10]:\n",
    "    nb_model = MultinomialNB(alpha=alpha)\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "\n",
    "    print(f\"Classification report for alpha={alpha}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    with open(f'nb_alpha{alpha}_model.pkl', 'wb') as file:\n",
    "        pickle.dump(nb_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dde3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee013177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b74ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b8b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
